{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In order to load the stylesheet of this notebook, execute the last code cell in this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender System for Amazon Electronics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will be working with the [Amazon dataset](http://cs-people.bu.edu/kzhao/teaching/amazon_reviews_Electronics.tar.gz). You will build a recommender system to make predictions related to reviews of Electronics products on Amazon.\n",
    "\n",
    "Your grades will be determined by your performance on the predictive tasks as well as a brief written report about the approaches you took.\n",
    "\n",
    "This assignment should be completed **individually**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train.json** 1,000,000 reviews to be used for training. It is not necessary to use all reviews for training if doing so proves too computationally intensive. The fields in this file are:\n",
    "\n",
    "* **reviewerID** The ID of the reviewer. This is a hashed user identifier from Amazon.\n",
    "\n",
    "* **asin** The ID of the item. This is a hashed product identifier from Amazon.\n",
    "\n",
    "* **overall** The rating of reviewer gave the item.\n",
    "\n",
    "* **helpful** The helpfulness votes for the review. This has 2 subfields, 'nHelpful' and 'outOf'. The latter is the total number of votes this review received. The former is the number of those that considered the review to be helpful.\n",
    "\n",
    "* **reviewText** The text of the review.\n",
    "\n",
    "* **summary** The summary of the review.\n",
    "\n",
    "* **unixReviewTime** The time of the review in seconds since 1970."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**meta.json** Contains metadata of the items:\n",
    "\n",
    "* **asin** The ID of the item.\n",
    "\n",
    "* **categories** The category labels of the item being reviewed.\n",
    "\n",
    "* **price** The price of the item.\n",
    "\n",
    "* **brand** The brand of the item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pairs_Rating.txt** The pairs (reviewerID and asin) on which you are to predict ratings.\n",
    "\n",
    "**pairs_Purchase.txt** The pairs on which you are to predict whether a user purchased an item or not.\n",
    "\n",
    "**pairs_Helpful.txt** The pairs on which you are to predict helpfulness votes. A third column in this file is the total number of votes from which you should predict how many were helpful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**helpful.json** The review data associated with the helpfulness prediction test set. The 'nHelpful' field has been removed from this data since that is the value you need to predict above. This data will only be of use for the helpfulness prediction task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rating prediction** Predict people's star ratings as accurately as possible for those (reviewerID, asin) pairs in 'pairs_Rating.txt'. Accuracy will be measured in terms of the [root mean-squared error (RMSE)](http://www.kaggle.com/wiki/RootMeanSquaredError).\n",
    "\n",
    "**Purchase prediction** Predict given a (reviewerID, asin) pair from 'pairs_Purchase.txt' whether the user purchased the item (really, whether it was one of the items they reviewed). Accuracy will be measured in terms of the [categorization accuracy](http://www.kaggle.com/wiki/HammingLoss) (1 minus the Hamming loss).\n",
    "\n",
    "**Helpfulness prediction** Predic whether a user's review of an item will be considered helpful. The file 'pairs_Helpful.txt' contains (reviewerID, asin) pairs with a third column containing the number of votes the user's review of the item received. You must predict how many of them were helpful. Accuracy will be measured in terms of the total [absolute error](http://www.kaggle.com/wiki/AbsoluteError), i.e. you are penalized one according to the difference |nHelpful - prediction|, where 'nHelpful' is the number of helpful votes the review actually received, and 'prediction' is your prediction of this quantity.\n",
    "\n",
    "We set up competitions on Kaggle to keep track of your results compared to those of other members of the class. The leaderboard will show your results on half of the test data, but your ultimate score will depend on your predictions across the whole dataset.\n",
    "* Kaggle competition: [rating prediction](https://inclass.kaggle.com/c/cs591-hw3-rating-prediction3) click here to [join](https://kaggle.com/join/datascience16rating)\n",
    "* Kaggle competition: [purchase prediction](https://inclass.kaggle.com/c/cs591-hw3-purchase-prediction) click here to [join](https://kaggle.com/join/datascience16purchase)\n",
    "* Kaggle competition: [helpfulness prediction](https://inclass.kaggle.com/c/cs591-hw3-helpful-prediction) click here to [join](https://kaggle.com/join/datascience16helpful)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**baseline.py** A simple baseline for each task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will be graded on the following aspects.\n",
    "\n",
    "* Your written report. This should describe the approaches you took to each of the 3 tasks. To obtain good performance, you should not need to invent new approaches (though you are more than welcome to) but rather you will be graded based on your decision to apply reasonable approaches to each of the given tasks. (**10pts** for each task)\n",
    "\n",
    "* Your ability to obtain a solution which outperforms the baselines on the unseen portion of the test data. Obtaining full marks requires a solution which is substantially better (at least several percent) than baseline performance. (**10pts** for each task)\n",
    "\n",
    "* Your ranking for each of the three tasks compared to other students in the class. (**5pts** for each task)\n",
    "\n",
    "* Obtain a solution which outperforms the baselines on the seen portion of the test data (the leaderboard). \n",
    "(**5pts** for each task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple baselines have been provided for each of the 3 tasks. These are included in 'baselines.py' among the files above. These 3 baselines operate as follows:\n",
    "\n",
    "**Rating prediction** Returns the global average rating, or the user's average if you have seen them before in the training data.\n",
    "\n",
    "**Purchase prediction** Finds the most popular products that account for 50% of purchases in the training data. Return '1' whenever such a product is seen at test time, '0' otherwise.\n",
    "\n",
    "** Helpfulness prediction** Multiplies the number of votes by the global average helpfulness rate, or the user's rate if we saw this user in the training data.\n",
    "\n",
    "Running 'baseline.py' produces 3 files containing predicted outputs. Your submission files should have the same format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Citation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image-based recommendations on styles and substitutes** J. McAuley, C. Targett, J. Shi, A. van den Hengel *SIGIR*, 2015\n",
    "\n",
    "**Inferring networks of substitutable and complementary products** J. McAuley, R. Pandey, J. Leskovec *Knowledge Discovery and Data Mining*, 2015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written Report:**\n",
    "\n",
    "Kaggle name: A-Law\n",
    "\n",
    "Task 1: Improved version of the baseline\n",
    "\n",
    "After a few tries with matrix factorization and K-nearest neighboors, I ran into many issues with scalability and accuracy, where most of my iterations did not improve my orignal baseline submission. I decided to work with the orignal baseline algorithm and implemented more stringent conditions to improve the estimation. I made use of the meta file to categorize items and collect overall averages for each category, as well as collecting average ratings for each item available in the training. Then, based on the item averages, I looked at the average percent deviation a user has rated an item. This allows me to calculate the \"bias\" of a user over mutiple reviews from the average \"true\" score of an item. \n",
    "\n",
    "This improved baseline allows me to make better estimations in two ways. If both an item average ratings and the user's average rating percent deviation are known, then an estimation of the score can be made with both by calculating the product of the two. If the item is not in the training set, then the categorical average rating can be used. If the user is not in the training set, then the baseline method is to return the item average score if known, or the categorical score if the item is not in the training set.\n",
    "\n",
    "This prediction method has the advantage of being very scalable and useful to make basic assumptions about a user's rating habits and the distribution of ratings a product has given a user database. Potential improvments could include more specific calculations of deviation for each category that a user has a rated in. Another factor that can be included is the calcuation of sentiment in the summary text as a quick simple score that can be included as a factor in prediction. However, given the simplicity of the algorithm and how limited the training set is, there is few other ways that I can think of. It's simplicity is reflected in its score as well. I only manage to beat the original baseline by about 11%. However this was a good reflection of predictive systems in general: a fast and intuitive method is good to use as a baseline against more sophisiticated methods. \n",
    "\n",
    "Task 2: Bayesian Probabilities\n",
    "\n",
    "This method makes heavy use of item categories. Given that most purchases made by users would reasonably be complementary products i.e. speaker systems and amplifiers often go hand in hand, classifying purchase decisions by category can still make good predictions as long as categories are distinctive enough (which they are). Bayesian probability will thus involve conditional probabilities: given the probability that item A in category A is purchased alongside item B in category B, as well as the probability of either being brought, calculate the probablity that item B is purcased given that the user has purchased item A. These probabilities and categories are calculated from the meta and training dataset. \n",
    "\n",
    "Thus accurate purchase probabilities can be calculated given that a user's purchase history is known from the training set. If any of the purchases returns a probability score greater than 60%, then its more likely than not that a user would buy a product in that category and return a 1. This does seem to be effective in predictive accuracy. Scalability is also very good, as the algorithm relies on the 800 and so categories, rather than creating a much larger matrix of every item. Potential improvements on this algorithm would be to account for additional attributes if the probability is greater than 60%, such as the item's price compared to the average prices of items a user has purchased which could account for a user's purchasing power. This method also has no use if the user is not present in the training set: then the next best alternative is the baseline since there aren't other possible cases where the conditional probability is useful. However, where appliciable, the bayesian probability analysis has substantially improved the baseline.\n",
    "\n",
    "Task 3: User-Item Rating Deviation Penalization\n",
    "   \n",
    "This method is simplistic but effective. By analyzing the squared difference between a user's rating and the average rating of a product, we can introduce a penalization factor for helpfulness. This method is similar to the method employed in task 1, but the main difference is that the squared difference allows for greater penalty if the user rates the product further away from the average of all scores. This acts as a proxy for measuring the \"accuracy\" of a review for the product, and thus the smaller the absolute difference is between the user's and average rating is, the more useful a review would theoretically be, and thus higher the helpfulness score would be. \n",
    "\n",
    "The accuracy of the algorithm is substantial and I've managed to score well enough above the baseline without making large sacrifices in scalability. One way to potentially improve this algorithm would be to incorporate an analysis of the review text itself and it's length, i.e. a factor to incorporate review length into the helpfulness score, since presumably the longer a review is, the more helpful or less helpful it is in context of its accuracy in its sentiment towards the product. However this may lead to more extreme distributions of helpfulness scoress which could increase the absolute error. It may also be insightful to look into the user's main purchase history and see if past reviews have been more or less helpful in different categories; however, this may also be subject to bias and it would not be helpful if the user is not in the training data or if the item reviewed has never been in a category that the user has purchased from before. One advantage is that since all the data for each review except the helpfulness score is known, there is no need for a basic baseline since it's virtually guarenteed we have data on the user's own review score and the average score of the product. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 55.4195542336 secs\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Task 1: User Rating Prediction\"\"\"\n",
    "\n",
    "import json\n",
    "import time \n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_ratings_1(filename = \"amazon_reviews_Electronics/train.json\"):\n",
    "    '''Goal: using user averages, item averages, or categorical averages in descending order'''\n",
    "    \n",
    "    #first pass: categorize each item in meta.json \n",
    "    items_category = {}\n",
    "    category_rating = {}\n",
    "    with open('amazon_reviews_Electronics/meta.json','r') as f:\n",
    "        for line in f:\n",
    "            line = line.replace('\\\\','')\n",
    "            line = line.replace(\"'s\",'')\n",
    "            line = line.replace('7\" (','7 (')\n",
    "            line = line.replace('7\" S','7 S')\n",
    "            line = line.replace('(7\")','(7)')\n",
    "            line = line.replace('8.9\" ','8.9 ')\n",
    "            json_line = json.loads(line)\n",
    "            cat_name = ast.literal_eval(json_line['categories'])[0]\n",
    "            cat_name = '/'.join(cat_name)\n",
    "            itemID = json_line['asin']\n",
    "            items_category[itemID] = cat_name\n",
    "            if cat_name not in category_rating:\n",
    "                category_rating[cat_name] = []\n",
    "    \n",
    "    #second pass: create item_dict which stores all their corresponding ratings to compute average\n",
    "    item_rating = {}\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "            #first 174 chars needed, nothing else\n",
    "            line = line[0:173] + '}'\n",
    "            json_line = json.loads(line)\n",
    "            if json_line['asin'] not in item_rating:\n",
    "                item_rating[json_line['asin']] = [json_line['overall']]\n",
    "            else:\n",
    "                item_rating[json_line['asin']] += [json_line['overall']]\n",
    "                \n",
    "            item_cat = items_category[json_line['asin']]\n",
    "            category_rating[item_cat] += [json_line['overall']]\n",
    "    \n",
    "    for item in item_rating: \n",
    "        item_rating[item] = np.average(item_rating[item])\n",
    "    \n",
    "    #third pass: find user average score deviation as a percentage\n",
    "    user_rating = {}\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "            #first 174 chars needed, nothing else\n",
    "            line = line[0:173] + '}'\n",
    "            json_line = json.loads(line)\n",
    "            itemrate = item_rating[json_line['asin']]\n",
    "            if json_line['reviewerID'] not in user_rating:\n",
    "                user_rating[json_line['reviewerID']] = [1.0 + ((json_line['overall'] - itemrate)/itemrate)]\n",
    "            else:\n",
    "                user_rating[json_line['reviewerID']] += [1.0 + ((json_line['overall'] - itemrate)/itemrate)]\n",
    "\n",
    "    for user in user_rating:\n",
    "        user_rating[user] = np.average(user_rating[user])\n",
    "        \n",
    "    #compute category averages\n",
    "    for cat in category_rating:\n",
    "        category_rating[cat] = np.average(category_rating[cat])\n",
    "        \n",
    "    #Items or Users that don't appear in training set = return category average\n",
    "    predictions = open(\"predictions_Rating.txt\", 'w')\n",
    "    with open(\"amazon_reviews_Electronics/pairs_Rating.txt\",'r') as f:\n",
    "        for line in f:\n",
    "            if \"reviewerID-asin\" in line:\n",
    "                #header\n",
    "                predictions.write(line)\n",
    "                continue\n",
    "            else:\n",
    "                userID,itemID = line.strip().split('-')\n",
    "                try:\n",
    "                    user_deviation = user_rating[itemID]\n",
    "                    itemrate = item_rating[userID]\n",
    "                    prediction = itemrate * user_deviation\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        user_deviation = user_rating[itemID]\n",
    "                        prediction = category_rating[items_category[itemID]] * user_deviation\n",
    "                    except KeyError:\n",
    "                        try:\n",
    "                            prediction = item_rating[userID]\n",
    "                        except KeyError:\n",
    "                            prediction = category_rating[items_category[itemID]]\n",
    "                            \n",
    "            predictions.write(userID + '-' + itemID + ',' + str(prediction) + '\\n')\n",
    "        predictions.close()\n",
    "                \n",
    "start = time.time()\n",
    "parse_ratings_1()\n",
    "end = time.time() - start\n",
    "print('took ' + str(end) + ' secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 37.2049310207 secs\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Task 2: Purchase Prediction\"\"\"\n",
    "\n",
    "import json\n",
    "import time \n",
    "import ast\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "def parse_ratings_2(filename = \"amazon_reviews_Electronics/train.json\"):\n",
    "    \n",
    "    '''probabilistic analysis: \n",
    "        - goal: compute list of purchase decisions using training data\n",
    "        - for each category, compute percentile of complementary purchases \n",
    "        - P(buy item in different category|user has invested in these categories)\n",
    "        - return 1 if it is at least 50% \n",
    "    \n",
    "       Training:\n",
    "        - group items by their categories using meta.json\n",
    "            item: category\n",
    "        \n",
    "        - for each user, get categories that they purchased in and keep total count of each category and overall\n",
    "            user: {category:count}\n",
    "            category_count: count\n",
    "            overall_total = # of reviews\n",
    "        - convert each category count in each user to be a percentile of their total purchases\n",
    "            user: {category:count/total_count}\n",
    "            \n",
    "        - track items by top popularity in each category for use as improved baseline\n",
    "        \n",
    "        -for each category, create bayesian probabilities:\n",
    "        prob(cat_A|cat_B) = prob(cat_B|cat_A)*prob(cat_A) / prob(cat_B)\n",
    "        \n",
    "        \n",
    "       Testing:\n",
    "        -identify category of item\n",
    "        if user in user_dict\n",
    "            - find categories that they purchased in\n",
    "            - find max(prob(item_category|user_categories)),return 1 if > 0.5\n",
    "        if not then perform baseline if item in item_category\n",
    "        if not then default 0\n",
    "        '''\n",
    "    #record categories\n",
    "    categories_count = {}\n",
    "    items_category = {}\n",
    "    with open('amazon_reviews_Electronics/meta.json','r') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                line = line.replace('\\\\','')\n",
    "                line = line.replace(\"'s\",'')\n",
    "                line = line.replace('7\" (','7 (')\n",
    "                line = line.replace('7\" S','7 S')\n",
    "                line = line.replace('(7\")','(7)')\n",
    "                line = line.replace('8.9\" ','8.9 ')\n",
    "                json_line = json.loads(line)\n",
    "            except ValueError:\n",
    "                print line\n",
    "                return 0\n",
    "            cat_name = ast.literal_eval(json_line['categories'])[0]\n",
    "            cat_name = '/'.join(cat_name)\n",
    "            itemID = json_line['asin']\n",
    "            items_category[itemID] = cat_name\n",
    "            if cat_name not in categories_count:\n",
    "                categories_count[cat_name] = 0\n",
    "    \n",
    "    #record purchasing decisions from training data for later use in creating P(A), P(B) and conditional prob\n",
    "    user_dict = {}\n",
    "    item_count = {}\n",
    "    total_count = 0.0\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "            #first 174 chars needed, nothing else\n",
    "            line = line[0:173] + '}'\n",
    "            json_line = json.loads(line)\n",
    "            item_cat = items_category[json_line['asin']]\n",
    "            if json_line['reviewerID'] not in user_dict:\n",
    "                user_dict[json_line['reviewerID']] = {item_cat:1,'total':1.0}\n",
    "            else:\n",
    "                if item_cat not in user_dict[json_line['reviewerID']]:\n",
    "                    user_dict[json_line['reviewerID']][item_cat] = 1\n",
    "                    user_dict[json_line['reviewerID']]['total'] += 1.0\n",
    "                else:\n",
    "                    user_dict[json_line['reviewerID']][item_cat] = 1\n",
    "                    user_dict[json_line['reviewerID']]['total'] += 1.0\n",
    "            if json_line['asin'] not in item_count:\n",
    "                item_count[json_line['asin']] = 1\n",
    "            else:\n",
    "                item_count[json_line['asin']] += 1\n",
    "            categories_count[item_cat] += 1\n",
    "            total_count += 1\n",
    "    \n",
    "    #convert category_count to percentiles\n",
    "    for cat in categories_count:\n",
    "        categories_count[cat] = categories_count[cat] / total_count\n",
    "    \n",
    "    #store median count\n",
    "    median_count = np.median(item_count.values())\n",
    "    \n",
    "    #convert user purchases into percentiles, remove total count\n",
    "    for user in user_dict:\n",
    "        for cat in user_dict[user]:\n",
    "            if cat == 'total':\n",
    "                continue\n",
    "            else:\n",
    "                user_dict[user][cat] = user_dict[user][cat]/user_dict[user]['total']\n",
    "        del user_dict[user]['total']\n",
    "    \n",
    "    #create category percentiles for each category (P(cat_A|cat_i) for all cat_i)\n",
    "    category_prob = {}\n",
    "    for user in user_dict:\n",
    "        for cat in user_dict[user]:\n",
    "            if cat not in category_prob:\n",
    "                category_prob[cat] = copy.deepcopy(user_dict[user])\n",
    "            else:\n",
    "                #ignore singular purchases\n",
    "                if len(user_dict[user]) > 1:\n",
    "                    for i in user_dict[user]:\n",
    "                        if i not in category_prob[cat]:\n",
    "                            category_prob[cat][i] = user_dict[user][i]\n",
    "                        else:\n",
    "                            category_prob[cat][i] += user_dict[user][i]\n",
    "    \n",
    "    #normalize probabilities in each category\n",
    "    for cat in category_prob:\n",
    "        cat_i = category_prob[cat][cat]\n",
    "        for cat_a in category_prob[cat]:\n",
    "            category_prob[cat][cat_a] = category_prob[cat][cat_a] / cat_i\n",
    "        del category_prob[cat][cat]\n",
    "    \n",
    "    #use probabilities to correct bayesian probablities in each category\n",
    "    \n",
    "    predictions = open(\"predictions_Purchase.txt\", 'w')\n",
    "    with open(\"amazon_reviews_Electronics/pairs_Purchase.txt\",'r') as f:\n",
    "        for line in f:\n",
    "            if \"reviewerID-asin\" in line:\n",
    "                #header\n",
    "                predictions.write(line)\n",
    "                continue\n",
    "            else:\n",
    "                userID,itemID = line.strip().split('-')\n",
    "                #if user has purchase history\n",
    "                item_cat = items_category[itemID][0]\n",
    "                try:\n",
    "                    user_categories = user_dict[userID]\n",
    "                    probabilities = []\n",
    "                    for category in user_categories:\n",
    "                        probabilities += [(category_prob[category][item_cat] * categories_count[item_cat]) / categories_count[category]]\n",
    "                    if max(probabilities) >= 0.6:\n",
    "                        prediction = 1\n",
    "                    else:\n",
    "                        prediction = 0\n",
    "                except KeyError:\n",
    "                    #no purchase history, perform baseline with known ratings\n",
    "                    try:\n",
    "                        itemcount = item_count[itemID]\n",
    "                        if itemcount > median_count:\n",
    "                            prediction = 1\n",
    "                        else:\n",
    "                            prediction = 0\n",
    "                \n",
    "                    except KeyError:\n",
    "                        #no known ratings or purchase history, default to 0\n",
    "                        prediction = 0\n",
    "            predictions.write(userID + '-' + itemID + ',' + str(prediction) + '\\n')\n",
    "        predictions.close()\n",
    "        \n",
    "start = time.time()\n",
    "parse_ratings_2()\n",
    "end = time.time() - start\n",
    "print('took ' + str(end) + ' secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 4.16630196571 secs\n"
     ]
    }
   ],
   "source": [
    "'''Task 3: Helpfulness Prediction'''\n",
    "\n",
    "def parse_ratings_3(filename = \"amazon_reviews_Electronics/helpful.json\"):\n",
    "    '''goal: compare user's overall rating to item average rating or category average and calculate the percent difference,\n",
    "        and account for the length of the review itself (longer=more helpful)\n",
    "       \n",
    "       training:\n",
    "       - group items by their categories using meta.json \n",
    "           item: category\n",
    "       - in training data, keep track of user-review: overall_rating/review_len, item:avg_rating, and category: avg_rating\n",
    "           userID:{itemID:[overall,review_len]}\n",
    "           item:avg_rating\n",
    "           category:avg_rating\n",
    "           \n",
    "       testing:\n",
    "       - 1 - ((average_rating - overall_rating)/average_rating)^2 * outOf'''\n",
    "    \n",
    "    user_reviews = {}\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "            json_line = json.loads(line)\n",
    "            if json_line['reviewerID'] not in user_reviews:\n",
    "                user_reviews[json_line['reviewerID']] = {json_line['asin']:json_line['overall']}\n",
    "            else:\n",
    "                user_reviews[json_line['reviewerID']][json_line['asin']] = json_line['overall']\n",
    "    \n",
    "    item_rating = {}\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "            #first 174 chars needed, nothing else\n",
    "            line = line[0:173] + '}'\n",
    "            json_line = json.loads(line)\n",
    "            if json_line['asin'] not in item_rating:\n",
    "                item_rating[json_line['asin']] = [json_line['overall']]\n",
    "            else:\n",
    "                item_rating[json_line['asin']] += [json_line['overall']]\n",
    "    \n",
    "    for item in item_rating:\n",
    "        item_rating[item] = np.average(item_rating[item])\n",
    "        \n",
    "    predictions = open(\"predictions_Helpful.txt\", 'w')\n",
    "    with open(\"amazon_reviews_Electronics/pairs_Helpful.txt\",'r') as f:\n",
    "        for line in f:\n",
    "            if \"reviewerID-asin\" in line:\n",
    "                #header\n",
    "                predictions.write(line)\n",
    "                continue\n",
    "            else:\n",
    "                userID,itemID,outOf = line.strip().split('-')\n",
    "                itemrate = item_rating[itemID]\n",
    "                overall = user_reviews[userID][itemID]\n",
    "                outOf = int(outOf)\n",
    "                prediction = (1.00 - ((itemrate-overall)/itemrate)**2)*outOf\n",
    "            predictions.write(userID + '-' + itemID + '-' + str(outOf) + ',' + str(prediction) + '\\n')\n",
    "        predictions.close()\n",
    "    \n",
    "start = time.time()\n",
    "parse_ratings_3()\n",
    "end = time.time() - start\n",
    "print('took ' + str(end) + ' secs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: '../theme/custom.css'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-a3b95627d7cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstyles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../theme/custom.css\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcss_styling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-63-a3b95627d7cd>\u001b[0m in \u001b[0;36mcss_styling\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcss_styling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mstyles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../theme/custom.css\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstyles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcss_styling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: '../theme/custom.css'"
     ]
    }
   ],
   "source": [
    "# Code for setting the style of the notebook\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"../theme/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
