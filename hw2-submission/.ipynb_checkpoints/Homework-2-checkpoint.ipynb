{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(In order to load the stylesheet of this notebook, execute the last code cell in this notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Reviews and Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will be working with the [Yelp dataset](http://cs-people.bu.edu/kzhao/teaching/yelp_dataset_challenge_academic_dataset.tar). You can find the format of the dataset [here](https://www.yelp.com/dataset_challenge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will look at Review Objects and perform some [sentiment analysis](http://sentiment.christopherpotts.net/) on the review text.\n",
    "\n",
    "You will need to preprocess the text using a stemming algorithm. The Porter stemming algorithm is a well-known one. Then, use a lexicon to assign a score to a review based on the positive/negative words you find in the text. You can find various lexicons [here](http://sentiment.christopherpotts.net/lexicons.html).\n",
    "\n",
    "After you have assigned scores to the reviews based on the text analysis, compare your scores with the stars associated with the reviews. **(20 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "read file\n",
    "read line by line to extract json object\n",
    "nltk sentiment intensity analyzer - takes in string returns sentiment\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "path = \"/Users/ALaw/Desktop/Stuff/2016 Spring/591/submissions/591-hw/hw2-submission/yelp_dataset_challenge_academic_dataset/\"\n",
    "filename = path + \"yelp_academic_dataset_review.json\"\n",
    "\n",
    "debug = False\n",
    "\n",
    "def parse(fn=filename):\n",
    "    '''Read line by line and extract (x,y) coordinates, where x = sentiment intensity and y = star rating'''\n",
    "    \n",
    "    data= []\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Begin Parsing...\")\n",
    "    \n",
    "    with open(fn) as f:\n",
    "        for line in f:\n",
    "            json_line = json.loads(line)\n",
    "            scores = sid.polarity_scores(json_line['text'])\n",
    "            sentiment = (scores['pos']*2 - scores['neg']*2)*(scores['compound']**2)\n",
    "            stars = json_line['stars']\n",
    "            data.append((stars,sentiment))\n",
    "    \n",
    "    \n",
    "    if debug:\n",
    "        print(\"Coverting to Dataframe...\")\n",
    "    \n",
    "    data = pd.DataFrame(data,columns = ['stars','sentiment'])\n",
    "    data = data.sort_values('stars')\n",
    "    \n",
    "    #print(list(data['stars']))\n",
    "    #print(list(data['sentiment']))\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Converting to CSV...\")\n",
    "    \n",
    "    data.to_csv('sample_data.csv')\n",
    "    \n",
    "    return data\n",
    "\n",
    "def visualize(data):\n",
    "    '''Visualizes the sentiment vs star rating data through a scatter plot and runs linear regression'''\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Sorting X,Y values...\")\n",
    "    \n",
    "    x = list(data['sentiment'])\n",
    "    y = list(data['stars'])\n",
    "    \n",
    "    plt.figure(figsize=(10,9))\n",
    "    plt.scatter(x, y)\n",
    "    plt.title('Review Data Scatterplot')\n",
    "    plt.ylabel('# of Stars')\n",
    "    plt.xlabel('Sentiment Score')\n",
    "    \n",
    "    if debug:\n",
    "        print(\"Graph complete\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "parse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization and short (detailed) analysis. **(10 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at Business Objects. Try to find culinary districts in Las Vegas. These are characterized by closeness and similarity of restaurants. Use the \"longitude\" and \"latitude\" to cluster closeness. Use \"categories\" and \"attributes\" to cluster for similarity.\n",
    "\n",
    "Find clusters using the 3 different techniques we discussed in class: k-means++, hierarchical, and GMM. Explain your data representation and how you determined certain parameters (for example, the number of clusters in k-means++). **(30 pts)**\n",
    "\n",
    "Things you may want to consider:\n",
    "1. The spatial coordinates and restaurant categories/attributes have different units of scale. Your results could be arbitrarily skewed if you don't incorporate some scaling.\n",
    "2. Some restaurant types are inherently more common than others. For example, there are probably lots of \"pizza\" restaurants. You may want to normalize your vectors so that you don't end up with only clusters of \"pizza\" restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize your clusters using each technique. Label your clusters. **(10 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's detect outliers. These are the ones who are the farthest from the centroids of their clusters. Track them down and describe any interesting observations that you can make. **(10 pts)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give a short (detailed) analysis comparing the 3 techniques. **(10 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Code for setting the style of the notebook\n",
    "from IPython.core.display import HTML\n",
    "def css_styling():\n",
    "    styles = open(\"../theme/custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
